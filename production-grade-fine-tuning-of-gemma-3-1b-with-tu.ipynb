{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false},{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239467,"modelId":222398}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"49e10010","cell_type":"markdown","source":"# Config-Driven Fine-Tuning of Gemma 3 1B on Kaggle TPU (Tunix/JAX)\n\nThis notebook implements an end-to-end **LoRA SFT → GRPO** pipeline for **Gemma 3 1B Instruct** using **Tunix** on **Kaggle TPU**.\n\nKey goals:\n- **No hard-coded hyperparameters**: all configuration is read from **`config.yaml`**.\n- Reasonable **small defaults** to validate the full pipeline quickly.\n- Save a **merged (base + LoRA) Hugging Face model** so you can **skip retraining** and run **inference** immediately.\n\nPrerequisites (Kaggle):\n1. Enable **TPU VM** in notebook settings.\n2. Attach a **Gemma 3 1B IT** weights dataset under `/kaggle/input/...` (Transformers format).\n3. Attach your training data under `/kaggle/input/...` (CSV/Parquet).\n\nNotes:\n- The **first GRPO step** includes **XLA compilation** and can be the slowest part of the run.\n- If `export/.../merged_lora/` already exists and `skip_training_if_export_exists: true`, training is skipped and the notebook goes straight to inference.\n","metadata":{}},{"id":"b54a9450","cell_type":"code","source":"\n# =========================\n# 0) Configuration (config.yaml)\n# =========================\n# All tunables live in config.yaml. This notebook reads them and assigns variables.\n# If config.yaml doesn't exist, we write a small \"smoke test\" config you can edit.\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Optional, Literal, List, Dict, Any\nimport os\n\n# YAML loader\ntry:\n    import yaml  # PyYAML\nexcept Exception as e:\n    raise RuntimeError(\n        \"Missing dependency: pyyaml. Install it with `pip install pyyaml` and re-run.\"\n    ) from e\n\n# Pydantic config models (recommended for validation)\ntry:\n    from pydantic import BaseModel, Field\nexcept Exception as e:\n    raise RuntimeError(\n        \"Missing dependency: pydantic. Install it with `pip install pydantic` and re-run.\"\n    ) from e\n\n\nDEFAULT_CONFIG_YAML = \"\"\"\nmodel:\n  family: gemma3\n  id: google/gemma-3-1b-it\n  # If null, we auto-search under /kaggle/input\n  local_dir: null\n  # Used only when auto-searching; a substring that should appear in the model path\n  prefer: gemma-3-1b-it\n  dtype: bfloat16\n  rank: 64\n  alpha: 64.0\n  lora_module_path: \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\"\n  eos_tokens: [1, 106]\n\ndata:\n  # If null, we auto-search under /kaggle/input for common train files (train.csv/train.parquet)\n  train_path: null\n  # Optional: if null we split from train\n  eval_path: null\n  val_frac: 0.02\n  seed: 123\n  max_train_rows: 5000\n  max_eval_rows: 500\n  q_col: null\n  a_col: null\n  r_col: null\n  id_col: null\n  system_prompt: \"You are a helpful assistant.\"\n  template: \"{system_prompt}\\n\\nQuestion: {question}\\n\\nAnswer: \"\n\nsft:\n  enabled: true\n  max_seq_len: 512\n  global_batch_size: 64\n  # learning rate scaling:\n  #   lr = lr_ref_global * global_batch_size / lr_ref_batch\n  lr_ref_global: 2.0e-5\n  lr_ref_batch: 64\n  max_steps: 50\n  eval_every_steps: 25\n  weight_decay: 0.05\n  b1: 0.9\n  b2: 0.99\n  max_examples: 4000\n\ngrpo:\n  enabled: true\n  # Keep these small for pipeline validation\n  max_prompt_length: 256\n  total_generation_steps: 128\n  safety_max_prompt_length: 512\n  safety_max_generation_steps: 256\n  temperature: 0.9\n  top_p: 1.0\n  top_k: 50\n  num_generations: 8\n  num_iterations: 1\n  beta: 0.04\n  epsilon: 0.2\n  max_steps: 300\n  eval_every_steps: 50\n  warmup_steps: 30\n  learning_rate: 5.0e-6\n  b1: 0.9\n  b2: 0.95\n  weight_decay: 0.05\n  max_grad_norm: 1.0\n  train_micro_batch_size: 4\n  rollout_micro_batch_pref: 1\n  compute_logps_micro_batch_pref: 1\n  offload_to_cpu: false\n  kv_cache_extra: 256\n  max_train_examples: 2000\n  max_eval_examples: 200\n\nexport:\n  enabled: true\n  export_root: \"/kaggle/working/export\"\n  run_name: \"google_gemma-3-1b-it_tunix_sft_grpo\"\n  merged_subdir: \"merged_lora\"\n  save_after_sft: true\n  save_after_grpo: true\n  save_interval_steps: 100\n  max_to_keep: 2\n\ninference:\n  enabled: true\n  # If null, defaults to export_root/run_name/merged_subdir\n  model_dir: null\n  max_new_tokens: 256\n  do_sample: true\n\nruntime:\n  require_tpu: true\n  mesh_fsdp: 8\n  mesh_tp: 1\n  skip_training_if_export_exists: true\n  force_retrain: false\n  silence_asyncio_noise: true\n\"\"\"\n\n\nclass ModelCfg(BaseModel):\n    family: Literal[\"gemma3\", \"gemma2\"] = \"gemma3\"\n    id: str = \"google/gemma-3-1b-it\"\n    local_dir: Optional[str] = None\n    prefer: Optional[str] = \"gemma-3-1b-it\"\n    dtype: Literal[\"bfloat16\", \"float16\", \"float32\"] = \"bfloat16\"\n    rank: int = 64\n    alpha: float = 64.0\n    lora_module_path: str = \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\"\n    eos_tokens: List[int] = Field(default_factory=lambda: [1, 106])\n\n\nclass DataCfg(BaseModel):\n    train_path: Optional[str] = None\n    eval_path: Optional[str] = None\n    val_frac: float = 0.02\n    seed: int = 123\n    max_train_rows: int = 5000\n    max_eval_rows: int = 500\n    q_col: Optional[str] = None\n    a_col: Optional[str] = None\n    r_col: Optional[str] = None\n    id_col: Optional[str] = None\n    system_prompt: str = \"You are a helpful assistant.\"\n    template: str = \"{system_prompt}\\n\\nQuestion: {question}\\n\\nAnswer: \"\n\n\nclass SFTCfg(BaseModel):\n    enabled: bool = True\n    max_seq_len: int = 512\n    global_batch_size: int = 64\n    lr_ref_global: float = 2.0e-5\n    lr_ref_batch: int = 64\n    max_steps: int = 50\n    eval_every_steps: int = 25\n    weight_decay: float = 0.05\n    b1: float = 0.9\n    b2: float = 0.99\n    max_examples: int = 4000\n\n    def learning_rate(self) -> float:\n        # Scale LR linearly with global batch size\n        return float(self.lr_ref_global) * float(self.global_batch_size) / float(self.lr_ref_batch)\n\n\nclass GRPOCfg(BaseModel):\n    enabled: bool = True\n    max_prompt_length: int = 256\n    total_generation_steps: int = 128\n    safety_max_prompt_length: int = 512\n    safety_max_generation_steps: int = 256\n    temperature: float = 0.9\n    top_p: float = 1.0\n    top_k: int = 50\n    num_generations: int = 8\n    num_iterations: int = 1\n    beta: float = 0.04\n    epsilon: float = 0.2\n    max_steps: int = 300\n    eval_every_steps: int = 50\n    warmup_steps: int = 30\n    learning_rate: float = 5.0e-6\n    b1: float = 0.9\n    b2: float = 0.95\n    weight_decay: float = 0.05\n    max_grad_norm: Optional[float] = 1.0\n    train_micro_batch_size: int = 4\n    rollout_micro_batch_pref: int = 1\n    compute_logps_micro_batch_pref: int = 1\n    offload_to_cpu: bool = False\n    kv_cache_extra: int = 256\n    max_train_examples: int = 2000\n    max_eval_examples: int = 200\n\n\nclass ExportCfg(BaseModel):\n    enabled: bool = True\n    export_root: str = \"/kaggle/working/export\"\n    run_name: str = \"google_gemma-3-1b-it_tunix_sft_grpo\"\n    merged_subdir: str = \"merged_lora\"\n    save_after_sft: bool = True\n    save_after_grpo: bool = True\n    save_interval_steps: int = 100\n    max_to_keep: int = 2\n\n\nclass InferenceCfg(BaseModel):\n    enabled: bool = True\n    model_dir: Optional[str] = None\n    max_new_tokens: int = 256\n    do_sample: bool = True\n\n\nclass RuntimeCfg(BaseModel):\n    require_tpu: bool = True\n    mesh_fsdp: int = 8\n    mesh_tp: int = 1\n    skip_training_if_export_exists: bool = True\n    force_retrain: bool = False\n    silence_asyncio_noise: bool = True\n\n\nclass NotebookCfg(BaseModel):\n    model: ModelCfg = Field(default_factory=ModelCfg)\n    data: DataCfg = Field(default_factory=DataCfg)\n    sft: SFTCfg = Field(default_factory=SFTCfg)\n    grpo: GRPOCfg = Field(default_factory=GRPOCfg)\n    export: ExportCfg = Field(default_factory=ExportCfg)\n    inference: InferenceCfg = Field(default_factory=InferenceCfg)\n    runtime: RuntimeCfg = Field(default_factory=RuntimeCfg)\n\n\nCONFIG_PATH = Path(\"config.yaml\")\n\nif not CONFIG_PATH.exists():\n    CONFIG_PATH.write_text(DEFAULT_CONFIG_YAML.strip() + \"\\n\", encoding=\"utf-8\")\n    print(f\"✅ Wrote default config.yaml to: {CONFIG_PATH.resolve()}\")\n    print(\"   Edit it (especially data.train_path / model.local_dir) and re-run this cell if needed.\")\n\ncfg_dict = yaml.safe_load(CONFIG_PATH.read_text(encoding=\"utf-8\"))\ncfg = NotebookCfg.model_validate(cfg_dict)\n\nprint(\"✅ Loaded config.yaml\")\nprint(\"  model:\", cfg.model.family, \"|\", cfg.model.id, \"| rank:\", cfg.model.rank)\nprint(\"  data: train_path:\", cfg.data.train_path, \"| val_frac:\", cfg.data.val_frac)\nprint(\"  sft: enabled:\", cfg.sft.enabled, \"| steps:\", cfg.sft.max_steps, \"| lr:\", cfg.sft.learning_rate())\nprint(\"  grpo: enabled:\", cfg.grpo.enabled, \"| steps:\", cfg.grpo.max_steps, \"| G:\", cfg.grpo.num_generations)\nprint(\"  export:\", cfg.export.export_root, \"/\", cfg.export.run_name, \"/\", cfg.export.merged_subdir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8edaffd","cell_type":"code","source":"\n# =========================\n# 1) Assign notebook variables from cfg (compat layer)\n# =========================\nfrom pathlib import Path\n\n# Paths\nKAGGLE_INPUT = Path(\"/kaggle/input\")\nKAGGLE_WORKING = Path(\"/kaggle/working\")\n\nRUN_NAME = cfg.export.run_name\n\nEXPORT_ROOT = Path(cfg.export.export_root)\nRUN_EXPORT_DIR = EXPORT_ROOT / RUN_NAME\nMERGED_LORA_DIR = RUN_EXPORT_DIR / cfg.export.merged_subdir\n\nTB_DIR = KAGGLE_WORKING / \"tensorboard\" / RUN_NAME\nCKPT_DIR = KAGGLE_WORKING / \"checkpoints\" / RUN_NAME\n\nfor p in [TB_DIR, CKPT_DIR, RUN_EXPORT_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\n# Model\nMODEL_FAMILY = cfg.model.family\nMODEL_ID = cfg.model.id\nRANK = int(cfg.model.rank)\nALPHA = float(cfg.model.alpha)\nLORA_MODULE_PATH = cfg.model.lora_module_path\nDTYPE_STR = cfg.model.dtype\n\n# Data\nVAL_FRAC = float(cfg.data.val_frac)\nq_col = cfg.data.q_col\na_col = cfg.data.a_col\nr_col = cfg.data.r_col\nid_col = cfg.data.id_col\nSYSTEM_PROMPT = cfg.data.system_prompt\nTEMPLATE = cfg.data.template\n\n# SFT\nRUN_SFT = bool(cfg.sft.enabled)\nSFT_MAX_SEQ_LEN = int(cfg.sft.max_seq_len)\nSFT_GLOBAL_BATCH_SIZE = int(cfg.sft.global_batch_size)\nSFT_LEARNING_RATE = float(cfg.sft.learning_rate())\nSFT_MAX_STEPS = int(cfg.sft.max_steps)\nSFT_EVAL_EVERY = int(cfg.sft.eval_every_steps)\nSFT_WEIGHT_DECAY = float(cfg.sft.weight_decay)\nSFT_B1 = float(cfg.sft.b1)\nSFT_B2 = float(cfg.sft.b2)\nSFT_MAX_EXAMPLES = int(cfg.sft.max_examples)\n\n# GRPO\nRUN_GRPO = bool(cfg.grpo.enabled)\nMAX_PROMPT_LENGTH = int(cfg.grpo.max_prompt_length)\nTOTAL_GENERATION_STEPS = int(cfg.grpo.total_generation_steps)\nTEMPERATURE = float(cfg.grpo.temperature)\nTOP_P = float(cfg.grpo.top_p)\nTOP_K = int(cfg.grpo.top_k)\nNUM_GENERATIONS = int(cfg.grpo.num_generations)\nNUM_ITERATIONS = int(cfg.grpo.num_iterations)\nBETA = float(cfg.grpo.beta)\nEPSILON = float(cfg.grpo.epsilon)\n\nMAX_STEPS = int(cfg.grpo.max_steps)\nEVAL_EVERY_N_STEPS = int(cfg.grpo.eval_every_steps)\nWARMUP_STEPS = int(cfg.grpo.warmup_steps)\nLEARNING_RATE = float(cfg.grpo.learning_rate)\nB1 = float(cfg.grpo.b1)\nB2 = float(cfg.grpo.b2)\nWEIGHT_DECAY = float(cfg.grpo.weight_decay)\nMAX_GRAD_NORM = cfg.grpo.max_grad_norm\n\nTRAIN_MICRO_BATCH_SIZE = int(cfg.grpo.train_micro_batch_size)\nROLLOUT_MB_PREF = int(cfg.grpo.rollout_micro_batch_pref)\nLOGPS_MB_PREF = int(cfg.grpo.compute_logps_micro_batch_pref)\nOFFLOAD_TO_CPU = bool(cfg.grpo.offload_to_cpu)\n\nSAFE_KV_CACHE_EXTRA = int(cfg.grpo.kv_cache_extra)\nGRPO_SAFETY_MAX_PROMPT_LENGTH = int(cfg.grpo.safety_max_prompt_length)\nGRPO_SAFETY_MAX_GEN_STEPS = int(cfg.grpo.safety_max_generation_steps)\n\nGRPO_MAX_TRAIN_EXAMPLES = int(cfg.grpo.max_train_examples)\nGRPO_MAX_EVAL_EXAMPLES = int(cfg.grpo.max_eval_examples)\n\n# Runtime\nREQUIRE_TPU = bool(cfg.runtime.require_tpu)\nMESH_FSDP = int(cfg.runtime.mesh_fsdp)\nMESH_TP = int(cfg.runtime.mesh_tp)\nSKIP_IF_EXPORT_EXISTS = bool(cfg.runtime.skip_training_if_export_exists)\nFORCE_RETRAIN = bool(cfg.runtime.force_retrain)\nSILENCE_ASYNCIO_NOISE = bool(cfg.runtime.silence_asyncio_noise)\n\n# Inference\nRUN_INFERENCE = bool(cfg.inference.enabled)\nINFER_DIR = Path(cfg.inference.model_dir) if cfg.inference.model_dir else MERGED_LORA_DIR\nINFER_MAX_NEW_TOKENS = int(cfg.inference.max_new_tokens)\nINFER_DO_SAMPLE = bool(cfg.inference.do_sample)\n\nprint(\"✅ Variables assigned from config.yaml via Pydantic.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eae28e6b","cell_type":"code","source":"\n# =========================\n# 2) Print configuration summary (for reproducibility)\n# =========================\nimport json\nfrom pprint import pprint\n\nprint(\"=== Run Identifiers ===\")\nprint(\"RUN_NAME:\", RUN_NAME)\nprint(\"EXPORT_DIR:\", RUN_EXPORT_DIR)\nprint(\"MERGED_LORA_DIR:\", MERGED_LORA_DIR)\nprint(\"TB_DIR:\", TB_DIR)\nprint(\"CKPT_DIR:\", CKPT_DIR)\n\nprint(\"\\n=== Model ===\")\nprint(\"MODEL_FAMILY:\", MODEL_FAMILY)\nprint(\"MODEL_ID:\", MODEL_ID)\nprint(\"DTYPE:\", DTYPE_STR)\nprint(\"LoRA rank/alpha:\", RANK, \"/\", ALPHA)\nprint(\"LoRA module_path:\", LORA_MODULE_PATH)\n\nprint(\"\\n=== Data ===\")\nprint(\"VAL_FRAC:\", VAL_FRAC)\nprint(\"q_col/a_col/r_col/id_col:\", q_col, a_col, r_col, id_col)\nprint(\"Max rows (train/eval):\", cfg.data.max_train_rows, \"/\", cfg.data.max_eval_rows)\n\nprint(\"\\n=== SFT ===\")\nprint(\"RUN_SFT:\", RUN_SFT)\nprint(\"SFT_MAX_SEQ_LEN:\", SFT_MAX_SEQ_LEN)\nprint(\"SFT_GLOBAL_BATCH_SIZE:\", SFT_GLOBAL_BATCH_SIZE)\nprint(\"SFT_LEARNING_RATE:\", SFT_LEARNING_RATE)\nprint(\"SFT_MAX_STEPS:\", SFT_MAX_STEPS)\nprint(\"SFT_EVAL_EVERY:\", SFT_EVAL_EVERY)\nprint(\"SFT_MAX_EXAMPLES:\", SFT_MAX_EXAMPLES)\n\nprint(\"\\n=== GRPO ===\")\nprint(\"RUN_GRPO:\", RUN_GRPO)\nprint(\"MAX_PROMPT_LENGTH:\", MAX_PROMPT_LENGTH)\nprint(\"GRPO_SAFETY_MAX_PROMPT_LENGTH:\", GRPO_SAFETY_MAX_PROMPT_LENGTH)\nprint(\"TOTAL_GENERATION_STEPS:\", TOTAL_GENERATION_STEPS)\nprint(\"GRPO_SAFETY_MAX_GEN_STEPS:\", GRPO_SAFETY_MAX_GEN_STEPS)\nprint(\"TEMPERATURE/TOP_P/TOP_K:\", TEMPERATURE, TOP_P, TOP_K)\nprint(\"NUM_GENERATIONS (G):\", NUM_GENERATIONS)\nprint(\"NUM_ITERATIONS (μ):\", NUM_ITERATIONS)\nprint(\"BETA/EPSILON:\", BETA, EPSILON)\nprint(\"MAX_STEPS:\", MAX_STEPS)\nprint(\"EVAL_EVERY_N_STEPS:\", EVAL_EVERY_N_STEPS)\nprint(\"TRAIN_MICRO_BATCH_SIZE:\", TRAIN_MICRO_BATCH_SIZE)\nprint(\"ROLLOUT_MB_PREF / LOGPS_MB_PREF:\", ROLLOUT_MB_PREF, \"/\", LOGPS_MB_PREF)\nprint(\"OFFLOAD_TO_CPU:\", OFFLOAD_TO_CPU)\n\nprint(\"\\n=== Inference ===\")\nprint(\"RUN_INFERENCE:\", RUN_INFERENCE)\nprint(\"INFER_DIR:\", INFER_DIR)\nprint(\"INFER_MAX_NEW_TOKENS:\", INFER_MAX_NEW_TOKENS)\nprint(\"INFER_DO_SAMPLE:\", INFER_DO_SAMPLE)\n\nprint(\"\\n=== Full config (dict) ===\")\npprint(cfg.model_dump())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"67458988","cell_type":"code","source":"\n# =========================\n# 3) Runtime setup (versions, seeding, logging)\n# =========================\nimport os\nimport random\nimport numpy as np\n\n# Reduce noisy logs (esp. ipykernel asyncio)\nimport logging\nimport warnings\n\nif SILENCE_ASYNCIO_NOISE:\n    logging.getLogger(\"asyncio\").setLevel(logging.CRITICAL)\n    warnings.filterwarnings(\"ignore\", message=r\".*coroutine.*was never awaited.*\")\n    warnings.filterwarnings(\"ignore\", message=r\".*Task was destroyed but it is pending.*\")\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    try:\n        import jax\n        jax.random.PRNGKey(seed)  # warms JAX RNG\n    except Exception:\n        pass\n\nseed_everything(cfg.data.seed)\n\nimport jax\nimport jax.numpy as jnp\n\nprint(\"Python:\", os.sys.version)\nprint(\"JAX:\", jax.__version__)\nprint(\"Backend:\", jax.default_backend())\nprint(\"Devices:\", len(jax.devices()), jax.devices()[:1])\n\nif REQUIRE_TPU and jax.default_backend() != \"tpu\":\n    raise RuntimeError(\n        \"This run requires TPU, but JAX backend is not TPU. \"\n        \"Enable TPU VM in Kaggle Notebook Settings, then restart the session.\"\n    )\n\ndef show_hbm_usage() -> None:\n    \"\"\"Best-effort per-device memory report (works on TPU in most Kaggle images).\"\"\"\n    for d in jax.devices():\n        try:\n            stats = d.memory_stats()\n            used = stats.get(\"bytes_in_use\", 0) / (1024**3)\n            limit = stats.get(\"bytes_limit\", 0) / (1024**3)\n            print(f\"{d}: {used:.2f} GiB / {limit:.2f} GiB ({(used/limit*100) if limit else 0:.1f}%)\")\n        except Exception:\n            print(f\"{d}: memory_stats() unavailable\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"36660add","cell_type":"code","source":"\n# =========================\n# (Optional) IPython display helpers\n# =========================\nfrom IPython.display import display\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d44c670b","cell_type":"code","source":"\n# =========================\n# 3.1) Dependency check (expected in Kaggle TPU images where Tunix is preinstalled)\n# =========================\nimport importlib\n\nrequired_modules = [\n    \"tunix\",\n    \"qwix\",\n    \"grain\",\n    \"orbax.checkpoint\",\n    \"transformers\",\n]\nmissing = []\nfor m in required_modules:\n    try:\n        importlib.import_module(m)\n    except Exception:\n        missing.append(m)\n\nif missing:\n    raise RuntimeError(\n        \"Missing required Python packages: \"\n        + \", \".join(missing)\n        + \"\\nIf you're on Kaggle with internet enabled, install with:\\n\"\n        + \"  pip install -q google-tunix[prod]==0.1.3 qwix grain orbax-checkpoint transformers\\n\"\n    )\n\nprint(\"✅ Dependencies look available.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"06aafc0b","cell_type":"code","source":"\n# =========================\n# 4) Create TPU mesh (AUTO axis types) — matches Tunix demo expectations\n# =========================\nfrom jax.sharding import AxisType\n\nassert len(jax.devices()) >= (MESH_FSDP * MESH_TP), (\n    f\"Need at least {MESH_FSDP*MESH_TP} devices for mesh, found {len(jax.devices())}.\"\n)\n\nif hasattr(jax, \"make_mesh\"):\n    mesh = jax.make_mesh(\n        (MESH_FSDP, MESH_TP),\n        (\"fsdp\", \"tp\"),\n        axis_types=(AxisType.Auto, AxisType.Auto),\n    )\nelse:\n    # Fallback: older JAX (axis_types may not be supported; if you hit sharding issues, upgrade JAX)\n    from jax.experimental import mesh_utils\n    from jax.sharding import Mesh\n    mesh = Mesh(mesh_utils.create_device_mesh((MESH_FSDP, MESH_TP)), (\"fsdp\", \"tp\"))\n\nprint(\"✅ Mesh:\", mesh)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0269e072","cell_type":"markdown","source":"## Data loading and preparation","metadata":{}},{"id":"d50027f5","cell_type":"code","source":"\n# =========================\n# 5) Locate + load training data (CSV/Parquet)\n# =========================\nfrom pathlib import Path\nimport pandas as pd\n\ndef find_first_file(root: Path, patterns: list[str]) -> Path | None:\n    root = Path(root)\n    for pat in patterns:\n        hits = sorted(root.rglob(pat))\n        if hits:\n            return hits[0]\n    return None\n\ndef load_table(path: Path) -> pd.DataFrame:\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Dataset file not found: {path}\")\n    if path.suffix.lower() == \".csv\":\n        return pd.read_csv(path)\n    if path.suffix.lower() in [\".parquet\", \".pq\"]:\n        return pd.read_parquet(path)\n    raise ValueError(f\"Unsupported file type: {path.suffix} (use CSV or Parquet)\")\n\n# Pick train/eval paths\ntrain_path = Path(cfg.data.train_path) if cfg.data.train_path else None\neval_path  = Path(cfg.data.eval_path)  if cfg.data.eval_path  else None\n\nif train_path is None:\n    # Auto-search common patterns in Kaggle inputs\n    train_path = find_first_file(\n        KAGGLE_INPUT,\n        patterns=[\n            \"**/train.csv\",\n            \"**/train.parquet\",\n            \"**/training.csv\",\n            \"**/training.parquet\",\n            \"**/*train*.csv\",\n            \"**/*train*.parquet\",\n        ],\n    )\n\nif train_path is None:\n    raise FileNotFoundError(\n        \"Could not auto-locate a training file under /kaggle/input. \"\n        \"Set data.train_path in config.yaml to an explicit CSV/Parquet path.\"\n    )\n\ntrain_df = load_table(train_path)\nprint(\"✅ Loaded train_df:\", train_path)\nprint(\"  rows:\", len(train_df), \"| cols:\", len(train_df.columns))\ndisplay(train_df.head(3))\n\n# Optional: cap rows for quick pipeline validation\nif cfg.data.max_train_rows and len(train_df) > cfg.data.max_train_rows:\n    train_df = train_df.sample(n=cfg.data.max_train_rows, random_state=cfg.data.seed).reset_index(drop=True)\n    print(f\"✅ Capped train_df to max_train_rows={cfg.data.max_train_rows}. New rows:\", len(train_df))\n\nif eval_path is not None and eval_path.exists():\n    eval_df = load_table(eval_path)\n    print(\"✅ Loaded eval_df:\", eval_path, \"| rows:\", len(eval_df))\n    if cfg.data.max_eval_rows and len(eval_df) > cfg.data.max_eval_rows:\n        eval_df = eval_df.sample(n=cfg.data.max_eval_rows, random_state=cfg.data.seed).reset_index(drop=True)\n        print(f\"✅ Capped eval_df to max_eval_rows={cfg.data.max_eval_rows}. New rows:\", len(eval_df))\nelse:\n    eval_df = None\n    print(\"ℹ️ eval_path not provided (or missing). We'll create val split from train_df.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b5c5d415","cell_type":"code","source":"\n# =========================\n# 6) Infer dataset schema (q_col / a_col / r_col / id_col)\n# =========================\nimport re\n\ndef to_str(x) -> str:\n    if x is None:\n        return \"\"\n    if isinstance(x, str):\n        return x\n    return str(x)\n\ndef guess_column(columns: list[str], candidates: list[str]) -> str | None:\n    cols = {c.lower(): c for c in columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    # fuzzy: contains substring\n    for c in columns:\n        cl = c.lower()\n        for cand in candidates:\n            if cand.lower() in cl:\n                return c\n    return None\n\ncols = list(train_df.columns)\n\n# Apply overrides if user set them in config\nif q_col is None:\n    q_col = guess_column(cols, [\"question\", \"prompt\", \"query\", \"input\", \"instruction\"])\nif a_col is None:\n    a_col = guess_column(cols, [\"answer\", \"output\", \"response\", \"completion\", \"solution\"])\nif r_col is None:\n    r_col = guess_column(cols, [\"reward\", \"score\", \"label\", \"target\"])\nif id_col is None:\n    id_col = guess_column(cols, [\"id\", \"uid\", \"uuid\", \"task_id\", \"example_id\"])\n\nprint(\"✅ Column mapping:\")\nprint(\"  q_col:\", q_col)\nprint(\"  a_col:\", a_col)\nprint(\"  r_col:\", r_col)\nprint(\"  id_col:\", id_col)\n\nif q_col is None:\n    raise ValueError(\n        \"Could not infer q_col (question/prompt). \"\n        \"Set data.q_col in config.yaml to the correct column name.\"\n    )\n\n# Basic sanity sample\nsample_rows = train_df[[q_col] + ([a_col] if a_col else [])].head(3)\ndisplay(sample_rows)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4ac2b7ad","cell_type":"code","source":"\n# =========================\n# 7) Train/val split (if eval_df not provided)\n# =========================\nimport numpy as np\n\nif eval_df is None:\n    rng = np.random.default_rng(cfg.data.seed)\n    idx = rng.permutation(len(train_df))\n    n_val = max(1, int(len(idx) * VAL_FRAC))\n    val_idx = idx[:n_val]\n    tr_idx  = idx[n_val:]\n    val_df = train_df.iloc[val_idx].reset_index(drop=True)\n    train_df_split = train_df.iloc[tr_idx].reset_index(drop=True)\nelse:\n    train_df_split = train_df\n    val_df = eval_df\n\nprint(\"✅ Split sizes:\")\nprint(\"  train_df_split:\", len(train_df_split))\nprint(\"  val_df:\", len(val_df))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"864ed29a","cell_type":"code","source":"\n# =========================\n# 8) Dataset diagnostics (what will be used and how)\n# =========================\nfrom collections import Counter\n\ndef build_prompt(q: str) -> str:\n    return TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=str(q))\n\n# How much data will SFT / GRPO actually consume?\nsft_n = min(SFT_MAX_EXAMPLES, len(train_df_split)) if RUN_SFT else 0\ngrpo_train_n = min(GRPO_MAX_TRAIN_EXAMPLES, len(train_df_split)) if RUN_GRPO else 0\ngrpo_val_n = min(GRPO_MAX_EVAL_EXAMPLES, len(val_df)) if RUN_GRPO else 0\n\nprint(\"=== Dataset usage plan ===\")\nprint(f\"Total train rows available: {len(train_df_split)}\")\nprint(f\"Total val rows available:   {len(val_df)}\")\nprint(\"\")\nprint(\"SFT seeding:\")\nprint(f\"  RUN_SFT={RUN_SFT}\")\nprint(f\"  Using up to: {sft_n} examples (cap: SFT_MAX_EXAMPLES={SFT_MAX_EXAMPLES})\")\nprint(f\"  Technique: supervised fine-tuning (teacher forcing) with LoRA adapters\")\nprint(\"\")\nprint(\"GRPO:\")\nprint(f\"  RUN_GRPO={RUN_GRPO}\")\nprint(f\"  Using up to: {grpo_train_n} train examples (cap: GRPO_MAX_TRAIN_EXAMPLES={GRPO_MAX_TRAIN_EXAMPLES})\")\nprint(f\"  Using up to: {grpo_val_n}   eval examples (cap: GRPO_MAX_EVAL_EXAMPLES={GRPO_MAX_EVAL_EXAMPLES})\")\nprint(f\"  Technique: reinforcement learning (GRPO) with num_generations={NUM_GENERATIONS}, num_iterations={NUM_ITERATIONS}\")\nprint(\"\")\n\n# Prompt length sanity (character-based, cheap)\nif len(train_df_split) > 0:\n    sample_prompts = [build_prompt(to_str(x)) for x in train_df_split[q_col].head(min(200, len(train_df_split)))]\n    lengths = np.array([len(p) for p in sample_prompts])\n    print(\"Prompt length (chars) on a small sample:\")\n    print(\"  min/median/max:\", int(lengths.min()), int(np.median(lengths)), int(lengths.max()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5a809ae1","cell_type":"markdown","source":"## Model loading (Gemma + Tunix) and LoRA setup","metadata":{}},{"id":"d49399c6","cell_type":"code","source":"\n# =========================\n# 9) Locate Gemma model directory (Transformers-style)\n# =========================\nfrom pathlib import Path\n\ndef find_model_dir(root: Path, prefer_substring: str | None = None) -> Path | None:\n    \"\"\"\n    Tries to locate a local HuggingFace Transformers model directory under `root`\n    by searching for config.json and one or more *.safetensors files.\n    \"\"\"\n    root = Path(root)\n    candidates: list[Path] = []\n    for cfg_path in root.rglob(\"config.json\"):\n        d = cfg_path.parent\n        has_weights = any(d.glob(\"*.safetensors\")) or (d / \"model.safetensors\").exists()\n        has_tokenizer = (d / \"tokenizer.model\").exists() or (d / \"tokenizer.json\").exists()\n        if has_weights and has_tokenizer:\n            candidates.append(d)\n\n    if not candidates:\n        return None\n\n    if prefer_substring:\n        prefer = prefer_substring.lower()\n        preferred = [c for c in candidates if prefer in str(c).lower()]\n        if preferred:\n            # prefer shortest path (more specific)\n            preferred.sort(key=lambda p: len(str(p)))\n            return preferred[0]\n\n    candidates.sort(key=lambda p: len(str(p)))\n    return candidates[0]\n\nlocal_model_path = Path(cfg.model.local_dir) if cfg.model.local_dir else None\nif local_model_path is None:\n    local_model_path = find_model_dir(KAGGLE_INPUT, prefer_substring=cfg.model.prefer)\n\nif local_model_path is None or not local_model_path.exists():\n    raise FileNotFoundError(\n        \"Could not locate Gemma weights under /kaggle/input. \"\n        \"Set model.local_dir in config.yaml to the exact Transformers directory \"\n        \"(the folder containing config.json + *.safetensors + tokenizer.*).\"\n    )\n\nprint(\"✅ local_model_path:\", local_model_path)\n\ntokenizer_path = local_model_path / \"tokenizer.model\"\nif not tokenizer_path.exists():\n    # Some models ship tokenizer.json instead; tokenizer_adapter supports tokenizer.model best.\n    print(\"⚠️ tokenizer.model not found. tokenizer_adapter may still work if tokenizer.json exists.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ac76d5b5","cell_type":"code","source":"\n# =========================\n# 10) Load base model on TPU (Tunix + safe tensors)\n# =========================\nimport qwix\nfrom flax import nnx\n\n# Tunix imports (Gemma family-specific)\nif MODEL_FAMILY == \"gemma3\":\n    from tunix.models.gemma3 import model as gemma_lib\n    from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n    from tunix.models.gemma3 import params as gemma_params\nelif MODEL_FAMILY == \"gemma2\":\n    from tunix.models.gemma2 import model as gemma_lib\n    from tunix.models.gemma2 import params as gemma_params\n    # Gemma2 typically uses orbax checkpoints in Tunix demos; safe tensors support may differ.\n    raise NotImplementedError(\"This notebook currently focuses on gemma3 safe-tensors loading.\")\nelse:\n    raise ValueError(f\"Unsupported MODEL_FAMILY={MODEL_FAMILY}\")\n\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n\n# dtype mapping\nDTYPE_MAP = {\n    \"bfloat16\": jnp.bfloat16,\n    \"float16\": jnp.float16,\n    \"float32\": jnp.float32,\n}\nDTYPE = DTYPE_MAP[DTYPE_STR]\n\nprint(\"\\n--- HBM BEFORE model load ---\")\nshow_hbm_usage()\n\n# Model config for Gemma3-1B\nmodel_config = gemma_lib.ModelConfig.gemma3_1b()\n\nwith mesh:\n    base_model = params_safetensors_lib.create_model_from_safe_tensors(\n        str(local_model_path),\n        model_config,\n        mesh,\n        dtype=DTYPE,\n    )\n\nprint(\"\\n--- HBM AFTER base model load ---\")\nshow_hbm_usage()\n\n# Tokenizer (prefer local tokenizer.model)\ntokenizer = tokenizer_lib.Tokenizer(tokenizer_path=str(tokenizer_path))\n\n# EOS tokens\nEOS_TOKENS = list(dict.fromkeys([*cfg.model.eos_tokens]))\nprint(\"EOS_TOKENS:\", EOS_TOKENS)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e21c2f0c","cell_type":"code","source":"\n# =========================\n# 11) Apply LoRA (critical: use model-provided sharded inputs)\n# =========================\nlora_provider = qwix.LoraProvider(\n    module_path=LORA_MODULE_PATH,\n    rank=RANK,\n    alpha=ALPHA,\n)\n\nwith mesh:\n    # Model-provided inputs ensure correct sharding for this model+mesh (prevents gather sharding ambiguity)\n    model_input = base_model.get_model_input()\n    lora_policy = qwix.apply_lora_to_model(\n        base_model,\n        lora_provider,\n        rngs=nnx.Rngs(params=0, lora=1),\n        **model_input,\n    )\n\nprint(\"✅ LoRA model created.\")\nprint(\"\\n--- HBM AFTER LoRA apply ---\")\nshow_hbm_usage()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8c442e43","cell_type":"code","source":"\n# =========================\n# 12) Skip logic: if exported merged model exists, skip training\n# =========================\ndef export_is_present(model_dir: Path) -> bool:\n    if not model_dir.exists():\n        return False\n    has_weights = any(model_dir.glob(\"*.safetensors\")) or (model_dir / \"model.safetensors\").exists()\n    has_cfg = (model_dir / \"config.json\").exists()\n    has_tokenizer = (model_dir / \"tokenizer.json\").exists() or (model_dir / \"tokenizer.model\").exists()\n    return bool(has_weights and has_cfg and has_tokenizer)\n\nEXPORT_PRESENT = export_is_present(MERGED_LORA_DIR)\nprint(\"EXPORT_PRESENT:\", EXPORT_PRESENT, \"|\", MERGED_LORA_DIR)\n\nif FORCE_RETRAIN:\n    print(\"⚠️ force_retrain=true: will retrain even if export exists.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de48bfd5","cell_type":"markdown","source":"## Stage 1 — LoRA SFT (format seeding)","metadata":{}},{"id":"7da50426","cell_type":"code","source":"# =========================\n# 13) Build SFT dataset (TrainingInput) — tokenized + masked\n# =========================\nimport numpy as np\nimport grain\nfrom typing import List\n\nfrom transformers import AutoTokenizer\n\n# --- FIX: Correct imports for Tunix SFT ---\n# sft_utils is not a direct module; we import 'utils' and alias it.\nfrom tunix.sft import peft_trainer\nfrom tunix.sft import utils as sft_utils \n\n# If your dataset has no answers, SFT is not meaningful.\nif a_col is None:\n    print(\"⚠️ a_col is None — SFT will be skipped (no supervised targets).\")\n    sft_ds = None\nelse:\n    # Local tokenizer (HF) for easy encoding; must be local_files_only=True in Kaggle offline mode.\n    hf_tok = AutoTokenizer.from_pretrained(str(local_model_path), local_files_only=True)\n    if hf_tok.pad_token_id is None:\n        # Gemma often has no pad token by default; using EOS as pad is a common workaround.\n        hf_tok.pad_token = hf_tok.eos_token\n\n    def tokenize_sft(prompt_text: str, answer_text: str) -> peft_trainer.TrainingInput:\n        # Full text contains prompt + answer + EOS\n        prompt_ids = hf_tok(prompt_text, add_special_tokens=False)[\"input_ids\"]\n        full_text = prompt_text + to_str(answer_text) + hf_tok.eos_token\n        full_ids = hf_tok(full_text, add_special_tokens=False)[\"input_ids\"]\n\n        # Mask: 0 for prompt, 1 for answer region\n        start = min(len(prompt_ids), len(full_ids))\n        mask = [0] * start + [1] * (len(full_ids) - start)\n\n        # Truncate\n        full_ids = full_ids[:SFT_MAX_SEQ_LEN]\n        mask = mask[:SFT_MAX_SEQ_LEN]\n\n        # Pad\n        pad_len = SFT_MAX_SEQ_LEN - len(full_ids)\n        if pad_len > 0:\n            full_ids += [hf_tok.pad_token_id] * pad_len\n            mask += [0] * pad_len\n\n        return peft_trainer.TrainingInput(\n            input_tokens=np.asarray(full_ids, dtype=np.int32),\n            input_mask=np.asarray(mask, dtype=np.int32),\n        )\n\n    # Pick subset for SFT seeding\n    # NOTE: Ensure 'train_df_split' and 'SFT_MAX_EXAMPLES' are defined above. \n    # If not, replace 'train_df_split' with 'train_df' and 'SFT_MAX_EXAMPLES' with 5000.\n    sft_n = min(SFT_MAX_EXAMPLES, len(train_df_split))\n    sft_rows = train_df_split.head(sft_n)\n\n    sft_inputs: List[peft_trainer.TrainingInput] = []\n    for _, row in sft_rows.iterrows():\n        q = to_str(row[q_col])\n        a = to_str(row[a_col])\n        prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n        sft_inputs.append(tokenize_sft(prompt, a))\n\n    print(f\"✅ Built SFT inputs: {len(sft_inputs)} examples | seq_len={SFT_MAX_SEQ_LEN}\")\n\n    # Grain dataset: shuffle + batch + repeat\n    # Fallback to seed=42 if 'cfg' is not defined\n    _seed = cfg.data.seed if 'cfg' in globals() else 42\n    sft_ds = grain.MapDataset.source(sft_inputs).shuffle(seed=_seed).batch(SFT_GLOBAL_BATCH_SIZE).repeat()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"da1beffa","cell_type":"code","source":"\n# =========================\n# 14) TRAINING: Stage 1 — LoRA SFT seeding\n# =========================\nimport optax\nfrom tunix.sft import metrics_logger\n\nRUN_SFT_EFFECTIVE = RUN_SFT and (sft_ds is not None) and not (EXPORT_PRESENT and SKIP_IF_EXPORT_EXISTS and not FORCE_RETRAIN)\n\nif not RUN_SFT_EFFECTIVE:\n    print(\"⚠️ Skipping SFT seeding.\")\n    if not RUN_SFT:\n        print(\"  - cfg.sft.enabled=false\")\n    elif sft_ds is None:\n        print(\"  - sft_ds is None (likely missing answers)\")\n    elif EXPORT_PRESENT and SKIP_IF_EXPORT_EXISTS and not FORCE_RETRAIN:\n        print(\"  - Exported model already present and skip_training_if_export_exists=true\")\nelse:\n    SFT_CKPT_DIR = CKPT_DIR / \"sft_seed\"\n    SFT_CKPT_DIR.mkdir(parents=True, exist_ok=True)\n\n    sft_logging_options = metrics_logger.MetricsLoggerOptions(\n        log_dir=str(TB_DIR / \"sft_seed\"),\n        flush_every_n_steps=10,\n    )\n\n    training_config = peft_trainer.TrainingConfig(\n        eval_every_n_steps=SFT_EVAL_EVERY,\n        max_steps=SFT_MAX_STEPS,\n        metrics_logging_options=sft_logging_options,\n        checkpoint_root_directory=str(SFT_CKPT_DIR),\n    )\n\n    def gen_model_input_fn(x: peft_trainer.TrainingInput):\n        pad_mask = x.input_tokens != hf_tok.pad_token_id\n        positions = sft_utils.build_positions_from_mask(pad_mask)\n        attention_mask = sft_utils.make_causal_attn_mask(pad_mask)\n        return {\n            \"input_tokens\": x.input_tokens,\n            \"input_mask\": x.input_mask,\n            \"positions\": positions,\n            \"attention_mask\": attention_mask,\n        }\n\n    sft_optimizer = optax.adamw(\n        learning_rate=SFT_LEARNING_RATE,\n        b1=SFT_B1,\n        b2=SFT_B2,\n        weight_decay=SFT_WEIGHT_DECAY,\n    )\n\n    sft_trainer = (\n        peft_trainer.PeftTrainer(lora_policy, sft_optimizer, training_config)\n        .with_gen_model_input_fn(gen_model_input_fn)\n    )\n\n    print(\"Starting SFT seeding...\")\n    print(\"Backend:\", jax.default_backend())\n    print(\"SFT_LEARNING_RATE:\", SFT_LEARNING_RATE, \"(global batch:\", SFT_GLOBAL_BATCH_SIZE, \")\")\n    print(\"SFT_MAX_STEPS:\", SFT_MAX_STEPS)\n\n    with mesh:\n        sft_trainer.train(sft_ds, None)\n\n    print(\"✅ SFT seeding complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"86430b66","cell_type":"code","source":"\n# =========================\n# 15) Export merged model (base + LoRA) for inference (HF-compatible directory)\n# =========================\nimport shutil\nimport json\nimport time\n\ndef export_merged_lora(output_dir: Path) -> Path:\n    \"\"\"\n    Writes a HuggingFace-style directory with merged LoRA weights.\n    Returns the output_dir path.\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Merge base + lora and save as safetensors\n    # (Gemma3 helper provided by Tunix)\n    gemma_params.save_lora_merged_model_as_safetensors(\n        str(local_model_path),\n        str(output_dir),\n        lora_policy,\n        rank=RANK,\n        alpha=ALPHA,\n    )\n\n    # Copy tokenizer/config artifacts expected by HF loaders\n    for fname in [\n        \"config.json\",\n        \"generation_config.json\",\n        \"tokenizer.model\",\n        \"tokenizer.json\",\n        \"tokenizer_config.json\",\n        \"special_tokens_map.json\",\n        \"added_tokens.json\",\n    ]:\n        src = local_model_path / fname\n        dst = output_dir / fname\n        if src.exists() and not dst.exists():\n            shutil.copy2(src, dst)\n\n    # Save run metadata\n    meta = {\n        \"run_name\": RUN_NAME,\n        \"model_id\": MODEL_ID,\n        \"local_model_path\": str(local_model_path),\n        \"dtype\": DTYPE_STR,\n        \"lora_rank\": RANK,\n        \"lora_alpha\": ALPHA,\n        \"sft\": cfg.sft.model_dump(),\n        \"grpo\": cfg.grpo.model_dump(),\n        \"data\": {\n            \"train_path\": str(train_path),\n            \"eval_path\": str(eval_path) if eval_path else None,\n            \"q_col\": q_col,\n            \"a_col\": a_col,\n            \"r_col\": r_col,\n            \"id_col\": id_col,\n            \"train_rows_used\": len(train_df_split),\n            \"val_rows_used\": len(val_df),\n        },\n        \"timestamp_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    }\n    (output_dir / \"tunix_run_metadata.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n\n    # Also store the exact config.yaml used\n    shutil.copy2(CONFIG_PATH, output_dir / \"config.yaml\")\n\n    return output_dir\n\nif cfg.export.enabled and cfg.export.save_after_sft and RUN_SFT_EFFECTIVE:\n    print(\"Exporting merged model after SFT to:\", MERGED_LORA_DIR)\n    out_dir = export_merged_lora(MERGED_LORA_DIR)\n    print(\"✅ Export complete:\", out_dir)\nelse:\n    print(\"ℹ️ Skipping export after SFT (either disabled or SFT didn't run).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"99fab841","cell_type":"markdown","source":"## Stage 2 — GRPO (reinforcement learning)","metadata":{}},{"id":"2fed8154","cell_type":"code","source":"\n# =========================\n# 16) Pre-GRPO dataset report (sizes, sampling, what will be used)\n# =========================\nprint(\"=== Pre-GRPO dataset report ===\")\nprint(\"Technique: GRPO (Group Relative Policy Optimization) on-policy RL with num_generations samples per prompt.\")\nprint(\"\")\n\nprint(\"Data sources:\")\nprint(\"  train_df_split rows:\", len(train_df_split))\nprint(\"  val_df rows:\", len(val_df))\nprint(\"\")\n\nprint(\"Configured caps:\")\nprint(\"  GRPO_MAX_TRAIN_EXAMPLES:\", GRPO_MAX_TRAIN_EXAMPLES)\nprint(\"  GRPO_MAX_EVAL_EXAMPLES:\", GRPO_MAX_EVAL_EXAMPLES)\nprint(\"\")\n\ntrain_use = min(GRPO_MAX_TRAIN_EXAMPLES, len(train_df_split))\nval_use   = min(GRPO_MAX_EVAL_EXAMPLES, len(val_df))\n\nprint(\"Will use:\")\nprint(\"  train examples:\", train_use, f\"({train_use/len(train_df_split)*100:.1f}% of available)\" if len(train_df_split) else \"\")\nprint(\"  eval examples:\",  val_use,   f\"({val_use/len(val_df)*100:.1f}% of available)\" if len(val_df) else \"\")\nprint(\"\")\n\nprint(\"Batching:\")\nprint(\"  TRAIN_MICRO_BATCH_SIZE:\", TRAIN_MICRO_BATCH_SIZE)\nprint(\"  NUM_GENERATIONS:\", NUM_GENERATIONS, \"=> full_batch_size =\", TRAIN_MICRO_BATCH_SIZE * NUM_GENERATIONS)\nprint(\"  MAX_PROMPT_LENGTH:\", MAX_PROMPT_LENGTH)\nprint(\"  TOTAL_GENERATION_STEPS:\", TOTAL_GENERATION_STEPS)\nprint(\"\")\n\n# Show a couple of samples\nif len(train_df_split) > 0:\n    _tmp = train_df_split[[q_col] + ([a_col] if a_col else [])].head(2)\n    display(_tmp)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"34a7f2e5","cell_type":"code","source":"\n# =========================\n# 17) GRPO reward functions (minimal, configurable)\n# =========================\nimport numpy as np\nimport re\n\ndef normalize_answer(s: str) -> str:\n    s = (s or \"\").strip()\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\ndef reward_format(prompts, completions, **kwargs):\n    \"\"\"\n    Reward +1 if completion contains something that looks like a final answer.\n    You should customize this to your task (e.g., JSON format, XML tags, etc.).\n    \"\"\"\n    rewards = []\n    for c in completions:\n        c = c or \"\"\n        has_text = len(c.strip()) > 0\n        rewards.append(1.0 if has_text else 0.0)\n    return np.asarray(rewards, dtype=np.float32)\n\ndef reward_answer(prompts, completions, answer=None, **kwargs):\n    \"\"\"\n    If ground-truth answers are present, reward exact normalized match.\n    If no answer column exists, returns zeros.\n    \"\"\"\n    if answer is None:\n        return np.zeros(len(completions), dtype=np.float32)\n\n    rewards = []\n    for c, a in zip(completions, answer):\n        c_norm = normalize_answer(c)\n        a_norm = normalize_answer(a)\n        rewards.append(1.0 if (a_norm and a_norm in c_norm) else 0.0)\n    return np.asarray(rewards, dtype=np.float32)\n\nREWARD_FNS = [reward_format, reward_answer]\nprint(\"✅ Reward functions registered:\", [fn.__name__ for fn in REWARD_FNS])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4eeff1af","cell_type":"code","source":"# =========================\n# 18) TRAINING: Stage 2 — GRPO (batch-safe + OOM-safe defaults)\n# =========================\nimport math\nimport numpy as np\nimport grain\nimport optax\nimport orbax.checkpoint as ocp\n\nfrom tunix.sft import metrics_logger\n\nfrom tunix.rl import rl_cluster as rl_cluster_lib\n# --- FIX: Import base_rollout from the correct submodule ---\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.rl.grpo.grpo_learner import GRPOLearner, GRPOConfig\n\ndef pick_divisor_microbatch(full_batch_size: int, preferred: int) -> int:\n    \"\"\"Largest <= preferred divisor of full_batch_size.\"\"\"\n    preferred = max(1, int(preferred))\n    for mb in range(preferred, 0, -1):\n        if full_batch_size % mb == 0:\n            return mb\n    return 1\n\nRUN_GRPO_EFFECTIVE = RUN_GRPO and not (EXPORT_PRESENT and SKIP_IF_EXPORT_EXISTS and not FORCE_RETRAIN)\n\nif not RUN_GRPO_EFFECTIVE:\n    print(\"⚠️ Skipping GRPO training.\")\n    if not RUN_GRPO:\n        print(\"  - cfg.grpo.enabled=false\")\n    elif EXPORT_PRESENT and SKIP_IF_EXPORT_EXISTS and not FORCE_RETRAIN:\n        print(\"  - Exported model already present and skip_training_if_export_exists=true\")\nelse:\n    # ------------------------------------------------------------\n    # A) Build train_dataset / val_dataset for GRPO (finite + repeat)\n    # ------------------------------------------------------------\n    # Cap examples for quick testing\n    train_use = min(GRPO_MAX_TRAIN_EXAMPLES, len(train_df_split))\n    val_use   = min(GRPO_MAX_EVAL_EXAMPLES, len(val_df))\n\n    def build_prompt(q: str) -> str:\n        return TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=str(q))\n\n    def make_example_from_row(row) -> dict:\n        q = to_str(row[q_col])\n        a = to_str(row[a_col]) if a_col is not None else \"\"\n        return {\"prompts\": build_prompt(q), \"question\": q, \"answer\": a}\n\n    train_examples = [make_example_from_row(r) for _, r in train_df_split.head(train_use).iterrows()]\n    val_examples   = [make_example_from_row(r) for _, r in val_df.head(val_use).iterrows()]\n\n    # Use cfg.data.seed if available, else default to 42\n    _seed = cfg.data.seed if 'cfg' in globals() else 42\n    train_base = grain.MapDataset.source(train_examples).shuffle(seed=_seed).batch(TRAIN_MICRO_BATCH_SIZE)\n    val_base   = grain.MapDataset.source(val_examples).batch(TRAIN_MICRO_BATCH_SIZE)\n\n    steps_per_epoch = len(train_base)\n    need_epochs = max(1, math.ceil(MAX_STEPS / max(1, steps_per_epoch)) + 1)\n\n    train_dataset = train_base.repeat(need_epochs)\n    val_dataset   = val_base.repeat(max(1, math.ceil((MAX_STEPS / max(1, len(val_base))) * 0.2) + 1))\n\n    print(\"✅ Built GRPO datasets\")\n    print(\"  train examples:\", len(train_examples), \"| batches/epoch:\", len(train_base))\n    print(\"  val examples:\", len(val_examples), \"| batches:\", len(val_base))\n    print(\"  MAX_STEPS:\", MAX_STEPS, \"| repeated epochs:\", need_epochs)\n\n    # ------------------------------------------------------------\n    # B) Length clamp (compile-time safety)\n    # ------------------------------------------------------------\n    SAFE_MAX_PROMPT_LENGTH = min(int(MAX_PROMPT_LENGTH), int(GRPO_SAFETY_MAX_PROMPT_LENGTH))\n    SAFE_GEN_STEPS = min(int(TOTAL_GENERATION_STEPS), int(GRPO_SAFETY_MAX_GEN_STEPS))\n    SAFE_KV_CACHE = SAFE_MAX_PROMPT_LENGTH + SAFE_GEN_STEPS + SAFE_KV_CACHE_EXTRA\n\n    print(\"GRPO length clamp:\")\n    print(\"  MAX_PROMPT_LENGTH:\", MAX_PROMPT_LENGTH, \"->\", SAFE_MAX_PROMPT_LENGTH)\n    print(\"  TOTAL_GENERATION_STEPS:\", TOTAL_GENERATION_STEPS, \"->\", SAFE_GEN_STEPS)\n    print(\"  kv_cache_size:\", SAFE_KV_CACHE)\n\n    # ------------------------------------------------------------\n    # C) Batch sizing constraints (divisibility)\n    # ------------------------------------------------------------\n    full_batch_size = int(TRAIN_MICRO_BATCH_SIZE) * int(NUM_GENERATIONS)\n\n    roll_mb = pick_divisor_microbatch(full_batch_size, preferred=ROLLOUT_MB_PREF)\n    logp_mb = pick_divisor_microbatch(full_batch_size, preferred=LOGPS_MB_PREF)\n\n    print(\"GRPO batch sizing:\")\n    print(\"  TRAIN_MICRO_BATCH_SIZE:\", TRAIN_MICRO_BATCH_SIZE)\n    print(\"  NUM_GENERATIONS:\", NUM_GENERATIONS)\n    print(\"  full_batch_size:\", full_batch_size)\n    print(\"  rollout_micro_batch_size:\", roll_mb)\n    print(\"  compute_logps_micro_batch_size:\", logp_mb)\n\n    # ------------------------------------------------------------\n    # D) Optimizer + configs\n    # ------------------------------------------------------------\n    checkpointing_options = ocp.CheckpointManagerOptions(\n        save_interval_steps=int(cfg.export.save_interval_steps),\n        max_to_keep=int(cfg.export.max_to_keep),\n    )\n    metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n        log_dir=str(TB_DIR / \"grpo\"),\n        flush_every_n_steps=10,\n    )\n\n    lr_schedule = optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    )\n\n    optimizer = optax.adamw(\n        learning_rate=lr_schedule,\n        b1=B1,\n        b2=B2,\n        weight_decay=WEIGHT_DECAY,\n    )\n    if MAX_GRAD_NORM is not None:\n        optimizer = optax.chain(optax.clip_by_global_norm(MAX_GRAD_NORM), optimizer)\n\n    cluster_config = rl_cluster_lib.ClusterConfig(\n        role_to_mesh={\n            rl_cluster_lib.Role.ACTOR: mesh,\n            rl_cluster_lib.Role.REFERENCE: mesh,\n            rl_cluster_lib.Role.ROLLOUT: mesh,\n        },\n        rollout_engine=\"vanilla\",\n        offload_to_cpu=OFFLOAD_TO_CPU,\n        training_config=rl_cluster_lib.RLTrainingConfig(\n            actor_optimizer=optimizer,\n            eval_every_n_steps=EVAL_EVERY_N_STEPS,\n            max_steps=MAX_STEPS,\n            mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n            train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n            rollout_micro_batch_size=roll_mb,\n            compute_logps_micro_batch_size=logp_mb,\n            data_sharding_axis=(\"fsdp\",),\n            metrics_logging_options=metrics_logging_options,\n            checkpoint_root_directory=str(CKPT_DIR / \"grpo\"),\n            checkpointing_options=checkpointing_options,\n        ),\n        rollout_config=base_rollout.RolloutConfig(\n            max_tokens_to_generate=SAFE_GEN_STEPS,\n            max_prompt_length=SAFE_MAX_PROMPT_LENGTH,\n            kv_cache_size=SAFE_KV_CACHE,\n            temperature=TEMPERATURE,\n            top_p=TOP_P,\n            top_k=TOP_K,\n            eos_tokens=EOS_TOKENS,\n        ),\n    )\n\n    algo_config = GRPOConfig(\n        num_generations=NUM_GENERATIONS,\n        num_iterations=NUM_ITERATIONS,\n        beta=BETA,\n        epsilon=EPSILON,\n    )\n\n    rl_cluster = rl_cluster_lib.RLCluster(\n        actor=lora_policy,\n        reference=base_model,\n        tokenizer=tokenizer,\n        cluster_config=cluster_config,\n    )\n\n    grpo_trainer = GRPOLearner(\n        rl_cluster=rl_cluster,\n        reward_fns=REWARD_FNS,\n        algo_config=algo_config,\n    )\n\n    print(\"Starting GRPO training...\")\n    print(\"  Note: first step includes XLA compilation and is typically the slowest.\")\n    with mesh:\n        grpo_trainer.train(train_dataset, val_dataset)\n\n    print(\"✅ GRPO training complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"55c098eb-a7f7-48b9-920c-1ba3ce3ed652","cell_type":"code","source":"print('is running')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1b80821","cell_type":"code","source":"\n# =========================\n# 19) Export merged model after GRPO (final artifact)\n# =========================\nif cfg.export.enabled and cfg.export.save_after_grpo and RUN_GRPO_EFFECTIVE:\n    print(\"Exporting merged model after GRPO to:\", MERGED_LORA_DIR)\n    out_dir = export_merged_lora(MERGED_LORA_DIR)\n    print(\"✅ Export complete:\", out_dir)\nelse:\n    print(\"ℹ️ Skipping export after GRPO (either disabled or GRPO didn't run).\")\n\n# Refresh export-present flag (useful if you ran training now)\nEXPORT_PRESENT = export_is_present(MERGED_LORA_DIR)\nprint(\"EXPORT_PRESENT:\", EXPORT_PRESENT, \"|\", MERGED_LORA_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"47f98ca3","cell_type":"markdown","source":"## Metrics and evaluation","metadata":{}},{"id":"816f250e","cell_type":"code","source":"\n# =========================\n# 20) Where are metrics logged?\n# =========================\nimport glob\n\nprint(\"TensorBoard root:\", TB_DIR)\nevent_files = glob.glob(str(TB_DIR / \"**\" / \"events.*\"), recursive=True)\nprint(\"Found event files:\", len(event_files))\nfor f in event_files[:10]:\n    print(\" -\", f)\n\nprint(\"\\nSFT metrics are written to:\", TB_DIR / \"sft_seed\")\nprint(\"GRPO metrics are written to:\", TB_DIR / \"grpo\")\nprint(\"\\nGRPO evaluation runs every EVAL_EVERY_N_STEPS =\", EVAL_EVERY_N_STEPS)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d01ec84","cell_type":"markdown","source":"## Inference (load the saved merged model and run predictions)","metadata":{}},{"id":"f90fac23","cell_type":"code","source":"\n# =========================\n# 21) Inference from merged model directory (no retraining required)\n# =========================\nfrom pathlib import Path\n\nif not export_is_present(INFER_DIR):\n    print(\"⚠️ Inference skipped: model directory not found or incomplete:\", INFER_DIR)\nelse:\n    print(\"✅ Loading model for inference from:\", INFER_DIR)\n\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n\n    tok = AutoTokenizer.from_pretrained(str(INFER_DIR), local_files_only=True)\n    if tok.pad_token_id is None:\n        tok.pad_token = tok.eos_token\n\n    # CPU inference (Kaggle TPU VMs typically don't have a CUDA GPU)\n    model = AutoModelForCausalLM.from_pretrained(\n        str(INFER_DIR),\n        local_files_only=True,\n        torch_dtype=torch.float32,\n        device_map=\"cpu\",\n    )\n    model.eval()\n\n    def generate_one(question: str) -> str:\n        prompt = TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n        inputs = tok(prompt, return_tensors=\"pt\")\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens=INFER_MAX_NEW_TOKENS,\n                do_sample=INFER_DO_SAMPLE,\n                temperature=TEMPERATURE if INFER_DO_SAMPLE else None,\n                top_p=TOP_P if INFER_DO_SAMPLE else None,\n                top_k=TOP_K if INFER_DO_SAMPLE else None,\n                pad_token_id=tok.pad_token_id,\n                eos_token_id=tok.eos_token_id,\n            )\n        return tok.decode(out[0], skip_special_tokens=True)\n\n    # Quick smoke test\n    test_q = to_str(train_df_split[q_col].iloc[0]) if len(train_df_split) else \"What is 2+2?\"\n    print(\"\\n--- Sample generation ---\")\n    print(generate_one(test_q))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"db365d19","cell_type":"markdown","source":"## Troubleshooting and practical tips\n\nCommon issues and the most effective fixes:\n\n- **GRPO seems “stuck” at the beginning**  \n  The first step includes XLA compilation for multiple graphs (rollout + log-prob computation). Reduce:\n  - `grpo.max_prompt_length`\n  - `grpo.total_generation_steps`\n  - `grpo.num_generations`\n  - `grpo.train_micro_batch_size`\n\n- **HBM OOM during reference log-prob computation**  \n  Set:\n  - `grpo.compute_logps_micro_batch_pref: 1`\n  - `grpo.offload_to_cpu: true` (slower, but reduces TPU HBM usage)\n\n- **You do not want to retrain**  \n  If `export/<run_name>/<merged_subdir>/` exists, keep:\n  - `runtime.skip_training_if_export_exists: true`  \n  and simply run the **Inference** section.\n\n- **Want to force a new run**  \n  Set:\n  - `runtime.force_retrain: true`\n","metadata":{}},{"id":"ff60056b-d839-4d46-913d-5c74d8e528d9","cell_type":"code","source":"print('is running')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"75aff97a-9915-4bb5-9575-89ee866550cd","cell_type":"code","source":"# =========================\n# 22) Enforce required output format: <reasoning>...</reasoning><answer>...</answer>\n# =========================\nimport re\n\nRE_STRICT = re.compile(r\"^<reasoning>.*?</reasoning><answer>.*?</answer>\\s*$\", re.DOTALL)\n\ndef sanitize_to_xml(text: str) -> str:\n    t = (text or \"\").strip()\n\n    # If already compliant\n    if RE_STRICT.match(t):\n        return t\n\n    # Try to extract \"Answer:\" and \"Explanation:\" style outputs\n    # 1) Answer\n    ans = \"\"\n    m_ans = re.search(r\"(?:^|\\n)Answer:\\s*(.*?)(?:\\n|$)\", t, re.IGNORECASE)\n    if m_ans:\n        ans = m_ans.group(1).strip()\n\n    # 2) Explanation/Reasoning\n    reason = \"\"\n    m_exp = re.search(r\"(?:^|\\n)Explanation:\\s*(.*)$\", t, re.IGNORECASE | re.DOTALL)\n    if m_exp:\n        reason = m_exp.group(1).strip()\n\n    # If still missing, fallback heuristics\n    if not ans:\n        # last non-empty line as answer\n        lines = [x.strip() for x in t.splitlines() if x.strip()]\n        ans = lines[-1] if lines else \"N/A\"\n\n    if not reason:\n        # everything except last line as reasoning\n        lines = [x.strip() for x in t.splitlines() if x.strip()]\n        reason = \"\\n\".join(lines[:-1]).strip() if len(lines) > 1 else \"I will reason step by step.\"\n\n    # Ensure non-empty answer\n    if not ans.strip():\n        ans = \"N/A\"\n\n    return f\"<reasoning>{reason}</reasoning><answer>{ans}</answer>\"\n\n# Quick check on your last generation\nsample_out = generate_one(test_q)\nprint(\"RAW:\\n\", sample_out[:500], \"...\\n\")\nprint(\"SANITIZED:\\n\", sanitize_to_xml(sample_out)[:500], \"...\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e80cbeec-1c14-4663-bfcd-8052f7d27e22","cell_type":"code","source":"# =========================\n# 22.4) Inspect /kaggle/input for submission/test files\n# =========================\nfrom pathlib import Path\n\nKAGGLE_INPUT = Path(\"/kaggle/input\")\n\ncsvs = sorted(KAGGLE_INPUT.rglob(\"*.csv\"))\njsons = sorted(KAGGLE_INPUT.rglob(\"*.json\"))\njsonls = sorted(KAGGLE_INPUT.rglob(\"*.jsonl\"))\nparquets = sorted(KAGGLE_INPUT.rglob(\"*.parquet\"))\n\nprint(\"CSV files found:\", len(csvs))\nfor p in csvs[:200]:\n    print(\" -\", p)\n\nprint(\"\\nJSON files found:\", len(jsons))\nfor p in jsons[:50]:\n    print(\" -\", p)\n\nprint(\"\\nJSONL files found:\", len(jsonls))\nfor p in jsonls[:50]:\n    print(\" -\", p)\n\nprint(\"\\nParquet files found:\", len(parquets))\nfor p in parquets[:50]:\n    print(\" -\", p)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a23ebbbd-6e55-4585-90ad-f77937b19a59","cell_type":"code","source":"# =========================\n# 22.5) Load test_df + sample_sub (robust, nonstandard filenames)\n# =========================\nfrom pathlib import Path\nimport pandas as pd\nimport re\n\nKAGGLE_INPUT = Path(\"/kaggle/input\")\n\ndef find_candidates(ext=\"csv\"):\n    return sorted(KAGGLE_INPUT.rglob(f\"*.{ext}\"))\n\ndef looks_like_sample_submission(df: pd.DataFrame) -> bool:\n    if df is None or df.shape[1] < 2:\n        return False\n    cols = [c.lower() for c in df.columns]\n    # heuristics: one id-like col + one pred-like col\n    id_like = any(any(k in c for k in [\"id\", \"qid\", \"uid\", \"index\"]) for c in cols)\n    return id_like\n\n# ---- Find a test file ----\ntest_path = None\nfor p in find_candidates(\"csv\"):\n    n = p.name.lower()\n    if \"test\" in n and \"train\" not in n and \"submission\" not in n:\n        test_path = p\n        break\n\nif test_path is None:\n    raise FileNotFoundError(\"Could not auto-find a test CSV under /kaggle/input. Use the listing cell output to set test_path manually.\")\n\nprint(\"✅ test_df:\", test_path)\ntest_df = pd.read_csv(test_path)\n\n# ---- Try to find a sample submission (any CSV with id-like cols) ----\nsample_sub = None\nsample_path = None\nfor p in find_candidates(\"csv\"):\n    n = p.name.lower()\n    if \"submission\" in n or \"sample\" in n:\n        try:\n            df = pd.read_csv(p)\n            if looks_like_sample_submission(df):\n                sample_sub = df\n                sample_path = p\n                break\n        except Exception:\n            pass\n\nif sample_sub is not None:\n    print(\"✅ sample_sub:\", sample_path)\n    print(\"sample_sub columns:\", list(sample_sub.columns))\nelse:\n    print(\"⚠️ No sample submission file found. Will create one from test_df.\")\n    # Try to find an id column in test_df\n    id_col_guess = None\n    for c in test_df.columns:\n        if c.lower() in [\"id\", \"qid\", \"uid\", \"index\"]:\n            id_col_guess = c\n            break\n    if id_col_guess is None:\n        # fallback: create an id as row index\n        test_df = test_df.reset_index().rename(columns={\"index\": \"id\"})\n        id_col_guess = \"id\"\n\n    # Create a default submission template with a placeholder prediction col name\n    sample_sub = pd.DataFrame({id_col_guess: test_df[id_col_guess].values, \"prediction\": \"\"})\n    print(\"Created sample_sub with columns:\", list(sample_sub.columns))\n\nprint(\"\\n--- test_df columns ---\")\nprint(list(test_df.columns))\nprint(test_df.head(2))\n\n# ---- Infer q_col if needed ----\ndef infer_question_col(df):\n    candidates = [\"question\", \"prompt\", \"query\", \"problem\", \"input\", \"text\", \"instruction\"]\n    cols = df.columns\n    for c in candidates:\n        if c in cols:\n            return c\n    for c in cols:\n        lc = c.lower()\n        if any(k in lc for k in candidates):\n            return c\n    # fallback: longest text-like col\n    best, best_len = None, -1\n    for c in cols:\n        s = df[c]\n        if s.dtype == \"object\":\n            avg = s.dropna().astype(str).head(200).map(len).mean()\n            if avg > best_len:\n                best_len = avg\n                best = c\n    return best\n\nif \"q_col\" not in globals() or q_col is None or q_col not in test_df.columns:\n    q_col = infer_question_col(test_df)\n    print(\"\\n✅ Inferred q_col for test_df:\", q_col)\n\nassert q_col in test_df.columns, f\"q_col='{q_col}' not found in test_df columns.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ecb42645-d2b0-423e-9a67-df7aff41593f","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bf49f909-5fe5-46b8-aa7a-cc6e855fe903","cell_type":"code","source":"# =========================\n# Zip merged_lora/ -> merged_lora.zip\n# =========================\nfrom pathlib import Path\nimport os, shutil, subprocess\n\nMERGED_DIR = Path(\"/kaggle/working/export/google_gemma-3-1b-it_tunix_sft_grpo/merged_lora\")\nZIP_BASENAME = Path(\"/kaggle/working/merged_lora\")  # .zip will be appended\n\nrequired = [\"config.json\", \"model.safetensors\", \"tokenizer.model\"]\nmissing = [f for f in required if not (MERGED_DIR / f).exists()]\nif missing:\n    raise FileNotFoundError(f\"Missing required files in {MERGED_DIR}: {missing}\")\n\n# Remove old zip if present\nzip_path = ZIP_BASENAME.with_suffix(\".zip\")\nif zip_path.exists():\n    zip_path.unlink()\n\n# Create zip (shutil.make_archive creates merged_lora.zip containing the folder)\narchive_path = shutil.make_archive(str(ZIP_BASENAME), \"zip\", root_dir=str(MERGED_DIR.parent), base_dir=MERGED_DIR.name)\nprint(\"✅ Created zip:\", archive_path)\nprint(\"Size (MB):\", round(Path(archive_path).stat().st_size / (1024**2), 2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f2bc034-0130-49e2-82b0-035c14fa68ed","cell_type":"code","source":"# =========================\n# 23) Batch inference → predictions.csv (id + prediction)\n# =========================\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# Ensure test_df and sample_sub exist\nassert test_df is not None and \"question\" in test_df.columns and \"id\" in test_df.columns\nassert sample_sub is not None and \"id\" in sample_sub.columns and \"prediction\" in sample_sub.columns\n\nRE_STRICT = re.compile(r\"^<reasoning>.*?</reasoning><answer>.*?</answer>\\s*$\", re.DOTALL)\n\ndef sanitize_to_xml(text: str) -> str:\n    t = (text or \"\").strip()\n\n    # already compliant\n    if RE_STRICT.match(t):\n        return t\n\n    # Try to capture \"Answer:\" and \"Explanation:\" style\n    ans = \"\"\n    m_ans = re.search(r\"(?:^|\\n)Answer:\\s*(.*?)(?:\\n|$)\", t, re.IGNORECASE)\n    if m_ans:\n        ans = m_ans.group(1).strip()\n\n    reason = \"\"\n    m_exp = re.search(r\"(?:^|\\n)Explanation:\\s*(.*)$\", t, re.IGNORECASE | re.DOTALL)\n    if m_exp:\n        reason = m_exp.group(1).strip()\n\n    # If model used GSM8K-style #### final\n    if not ans:\n        m_hash = re.findall(r\"####\\s*(.+)\", t)\n        if m_hash:\n            ans = m_hash[-1].strip()\n\n    # Fallback: last non-empty line\n    if not ans:\n        lines = [x.strip() for x in t.splitlines() if x.strip()]\n        ans = lines[-1] if lines else \"N/A\"\n\n    if not reason:\n        # everything before the last line (best effort)\n        lines = [x.strip() for x in t.splitlines() if x.strip()]\n        reason = \"\\n\".join(lines[:-1]).strip() if len(lines) > 1 else \"I will reason step by step.\"\n\n    if not ans.strip():\n        ans = \"N/A\"\n\n    return f\"<reasoning>{reason}</reasoning><answer>{ans}</answer>\"\n\n# Generate predictions\npreds = []\nfor q in tqdm(test_df[\"question\"].astype(str).tolist(), desc=\"Generating\"):\n    raw = generate_one(q)  # from your inference cell\n    preds.append(sanitize_to_xml(raw))\n\nsubmission = sample_sub.copy()\nsubmission[\"prediction\"] = preds\n\nout_path = \"/kaggle/working/predictions.csv\"\nsubmission.to_csv(out_path, index=False)\nprint(\"✅ Wrote:\", out_path)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea96698b-01e2-4f69-81ac-57b444e8d461","cell_type":"code","source":"## Load model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91d8ce8c-1452-4d0b-8e82-7ae36a9c547c","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"260ea895-7b7a-424e-a239-399a725a6d45","cell_type":"code","source":"# =========================\n# 0) Settings\n# =========================-\nfrom pathlib import Path\n\n# If you attached your merged model as a Kaggle Dataset, it will be under /kaggle/input/<dataset-slug>/merged_lora\n# Leave MODEL_DIR=None to auto-discover.\nMODEL_DIR = None  # e.g. Path(\"/kaggle/input/my-merged-model-dataset/merged_lora\")\n\n# Inference knobs (keep small-ish for speed on CPU)\nINFER_BATCH_SIZE = 4\nINFER_MAX_NEW_TOKENS = 256\nINFER_DO_SAMPLE = False   # deterministic\nINFER_TEMPERATURE = 0.7\nINFER_TOP_P = 0.95\nINFER_TOP_K = 50\n\n# Prompt\nSYSTEM_PROMPT = \"You are a helpful assistant.\"\n\n# Output\nSUBMISSION_PATH = Path(\"/kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:47:11.924003Z","iopub.execute_input":"2025-12-27T04:47:11.924335Z","iopub.status.idle":"2025-12-27T04:47:11.928263Z","shell.execute_reply.started":"2025-12-27T04:47:11.924311Z","shell.execute_reply":"2025-12-27T04:47:11.927449Z"}},"outputs":[],"execution_count":2},{"id":"66a17941-beaa-414d-a020-7ec86973870c","cell_type":"code","source":"# =========================\n# 1) Find merged model dir + verify it's complete\n# =========================\nfrom pathlib import Path\n\nKAGGLE_INPUT = Path(\"/kaggle/input\")\n\ndef find_hf_model_dirs(root: Path):\n    \"\"\"Return candidate HF model dirs under root that look like (config.json + weights + tokenizer).\"\"\"\n    cands = []\n    for p in root.rglob(\"*\"):\n        if not p.is_dir():\n            continue\n        has_config = (p / \"config.json\").exists()\n        has_weights = (p / \"model.safetensors\").exists() or (p / \"model.safetensors.index.json\").exists()\n        has_tok = any((p / f).exists() for f in [\"tokenizer.json\", \"tokenizer.model\", \"spiece.model\"])\n        if has_config and has_weights and has_tok:\n            cands.append(p)\n    # prefer shorter paths (more likely the intended directory)\n    return sorted(cands, key=lambda x: (len(str(x)), str(x)))\n\ndef verify_hf_model_dir(p: Path) -> tuple[bool, list[str]]:\n    missing = []\n    if not p.exists():\n        return False, [f\"Directory does not exist: {p}\"]\n    if not (p / \"config.json\").exists():\n        missing.append(\"config.json\")\n    has_weights = (p / \"model.safetensors\").exists() or (p / \"model.safetensors.index.json\").exists()\n    if not has_weights:\n        missing.append(\"model.safetensors OR model.safetensors.index.json (+ shards)\")\n    has_tokenizer = any((p / f).exists() for f in [\"tokenizer.json\", \"tokenizer.model\", \"spiece.model\"])\n    if not has_tokenizer:\n        missing.append(\"tokenizer.json OR tokenizer.model/spiece.model\")\n    return (len(missing) == 0), missing\n\nif MODEL_DIR is None:\n    cands = find_hf_model_dirs(KAGGLE_INPUT)\n    if not cands:\n        raise FileNotFoundError(\n            \"Could not find a HF model directory under /kaggle/input.\\n\"\n            \"Attach your merged_lora dataset, then rerun.\"\n        )\n    MODEL_DIR = cands[0]\n\nMODEL_DIR = Path(MODEL_DIR)\nok, missing = verify_hf_model_dir(MODEL_DIR)\n\nprint(\"MODEL_DIR:\", MODEL_DIR)\nprint(\"✅ Model folder looks usable.\" if ok else \"❌ Model folder incomplete:\")\nfor m in missing:\n    print(\" -\", m)\n\nassert ok, \"Fix MODEL_DIR (or dataset contents) and rerun.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:47:13.808054Z","iopub.execute_input":"2025-12-27T04:47:13.808394Z","iopub.status.idle":"2025-12-27T04:47:13.857456Z","shell.execute_reply.started":"2025-12-27T04:47:13.808370Z","shell.execute_reply":"2025-12-27T04:47:13.856717Z"}},"outputs":[{"name":"stdout","text":"MODEL_DIR: /kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\n✅ Model folder looks usable.\n","output_type":"stream"}],"execution_count":3},{"id":"8fff302c-52ea-4466-82dc-14e54050b5cf","cell_type":"code","source":"# =========================\n# 2) Load tokenizer + model (CPU/GPU auto)\n# =========================\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16 if device == \"cuda\" else torch.float32\n\nprint(\"Device:\", device, \"| dtype:\", dtype)\n\ntok = AutoTokenizer.from_pretrained(str(MODEL_DIR), local_files_only=True, use_fast=True)\nif tok.pad_token_id is None:\n    tok.pad_token = tok.eos_token\n\n# NOTE: transformers recently warns torch_dtype deprecated -> dtype; handle both.\nmodel_kwargs = dict(local_files_only=True)\ntry:\n    model = AutoModelForCausalLM.from_pretrained(str(MODEL_DIR), dtype=dtype, **model_kwargs)\nexcept TypeError:\n    model = AutoModelForCausalLM.from_pretrained(str(MODEL_DIR), torch_dtype=dtype, **model_kwargs)\n\nmodel.to(device)\nmodel.eval()\n\nprint(\"✅ Loaded model + tokenizer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:47:16.467345Z","iopub.execute_input":"2025-12-27T04:47:16.467719Z","iopub.status.idle":"2025-12-27T04:48:06.973738Z","shell.execute_reply.started":"2025-12-27T04:47:16.467695Z","shell.execute_reply":"2025-12-27T04:48:06.972834Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cpu | dtype: torch.float32\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded model + tokenizer.\n","output_type":"stream"}],"execution_count":4},{"id":"45761ab5-da80-46df-8ea0-e11be5fddd8d","cell_type":"code","source":"# =========================\n# 3) Prompt + batched generation helpers (chat_template-safe)\n# =========================\nimport torch\n\ndef build_prompt(question: str) -> str:\n    \"\"\"\n    Robust prompt builder:\n    - Tries tokenizer.chat_template with structured content (Gemma-style expects .text)\n    - Falls back to plain-string chat_template\n    - Falls back to a simple prompt\n    \"\"\"\n    question = str(question)\n\n    # Try chat template if available\n    if getattr(tok, \"chat_template\", None):\n        # 1) Structured content (most compatible with Gemma templates)\n        messages_structured = [\n            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}]},\n        ]\n        try:\n            return tok.apply_chat_template(\n                messages_structured,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            # 2) Plain string content (some templates accept this)\n            messages_plain = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": question},\n            ]\n            try:\n                return tok.apply_chat_template(\n                    messages_plain,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                )\n            except Exception:\n                pass\n\n    # 3) Hard fallback (always works)\n    return f\"{SYSTEM_PROMPT}\\nQuestion: {question}\\nAnswer:\"\n\n@torch.inference_mode()\ndef generate_batch(questions):\n    \"\"\"\n    Batched generation that returns ONLY the completion (not the prompt).\n    Works on CPU or GPU depending on `device`.\n    \"\"\"\n    prompts = [build_prompt(q) for q in questions]\n\n    inputs = tok(\n        prompts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n    ).to(device)\n\n    # prompt lengths per row (count of non-pad tokens)\n    prompt_lens = inputs[\"attention_mask\"].sum(dim=1).tolist()\n\n    gen_kwargs = dict(\n        max_new_tokens=int(INFER_MAX_NEW_TOKENS),\n        do_sample=bool(INFER_DO_SAMPLE),\n        pad_token_id=tok.pad_token_id,\n        eos_token_id=tok.eos_token_id,\n    )\n    if INFER_DO_SAMPLE:\n        gen_kwargs.update(dict(\n            temperature=float(INFER_TEMPERATURE),\n            top_p=float(INFER_TOP_P),\n            top_k=int(INFER_TOP_K),\n        ))\n\n    out = model.generate(**inputs, **gen_kwargs)\n\n    # Decode only newly generated tokens (not the prompt)\n    texts = []\n    for i in range(out.shape[0]):\n        start = int(prompt_lens[i])\n        completion_ids = out[i][start:]\n        texts.append(tok.decode(completion_ids, skip_special_tokens=True).strip())\n    return texts\n\n# Smoke test\nprint(generate_batch([\"What is 2+2?\"])[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:48:06.974344Z","iopub.execute_input":"2025-12-27T04:48:06.974834Z","iopub.status.idle":"2025-12-27T04:49:22.792898Z","shell.execute_reply.started":"2025-12-27T04:48:06.974814Z","shell.execute_reply":"2025-12-27T04:49:22.791616Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"2 + 2 = 4\nroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneroneronerone\n","output_type":"stream"}],"execution_count":5},{"id":"93dccd02-9833-4dc4-ade6-248fd9c8eb36","cell_type":"code","source":"# =========================\n# 4) Load test_df + sample_sub (robust)\n# =========================\nimport pandas as pd\nfrom pathlib import Path\n\nKAGGLE_INPUT = Path(\"/kaggle/input\")\n\n# Find sample_submission if it exists\nsample_paths = sorted(KAGGLE_INPUT.rglob(\"sample_submission.csv\"))\nsample_sub = None\nsample_path = None\nif sample_paths:\n    sample_path = sample_paths[0]\n    sample_sub = pd.read_csv(sample_path)\n    print(\"✅ sample_submission:\", sample_path)\nelse:\n    print(\"⚠️ sample_submission.csv not found. Will create a template from test_df.\")\n\n# Find a test CSV (common in competitions)\n# Prefer any file named test.csv; else any csv with \"test\" and not \"train\"\ncsvs = sorted(KAGGLE_INPUT.rglob(\"*.csv\"))\ntest_path = None\nfor p in csvs:\n    if p.name.lower() == \"test.csv\":\n        test_path = p\n        break\nif test_path is None:\n    for p in csvs:\n        n = p.name.lower()\n        if \"test\" in n and \"train\" not in n and \"submission\" not in n:\n            test_path = p\n            break\n\nif test_path is None:\n    raise FileNotFoundError(\"Could not locate a test CSV under /kaggle/input.\")\n\ntest_df = pd.read_csv(test_path)\nprint(\"✅ test_df:\", test_path)\nprint(\"test_df columns:\", list(test_df.columns))\ndisplay(test_df.head(2))\n\n# Infer q_col\ndef infer_question_col(df: pd.DataFrame) -> str:\n    candidates = [\"question\", \"prompt\", \"query\", \"problem\", \"input\", \"text\", \"instruction\"]\n    cols = list(df.columns)\n    lower_map = {c.lower(): c for c in cols}\n\n    # exact matches\n    for c in candidates:\n        if c in lower_map:\n            return lower_map[c]\n\n    # substring matches\n    for c in cols:\n        lc = c.lower()\n        if any(k in lc for k in candidates):\n            return c\n\n    # fallback: longest text-like\n    best, best_len = None, -1\n    for c in cols:\n        s = df[c]\n        if s.dtype == \"object\":\n            avg = s.dropna().astype(str).head(200).map(len).mean()\n            if avg > best_len:\n                best_len = avg\n                best = c\n    if best is None:\n        raise ValueError(\"Could not infer a question/prompt column. Set q_col manually.\")\n    return best\n\nq_col = infer_question_col(test_df)\nprint(\"✅ q_col:\", q_col)\n\n# Build sample_sub if missing\nif sample_sub is None:\n    # Prefer an id column if present\n    id_col = None\n    for c in test_df.columns:\n        if c.lower() in [\"id\", \"qid\", \"uid\", \"index\"]:\n            id_col = c\n            break\n    if id_col is None:\n        test_df = test_df.reset_index().rename(columns={\"index\": \"id\"})\n        id_col = \"id\"\n\n    sample_sub = pd.DataFrame({id_col: test_df[id_col].values, \"prediction\": \"\"})\n    sample_path = None\n    print(\"✅ Created sample_sub with columns:\", list(sample_sub.columns))\n\nprint(\"sample_sub columns:\", list(sample_sub.columns))\ndisplay(sample_sub.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:51:10.446057Z","iopub.execute_input":"2025-12-27T04:51:10.446372Z","iopub.status.idle":"2025-12-27T04:51:10.536745Z","shell.execute_reply.started":"2025-12-27T04:51:10.446353Z","shell.execute_reply":"2025-12-27T04:51:10.535965Z"}},"outputs":[{"name":"stdout","text":"⚠️ sample_submission.csv not found. Will create a template from test_df.\n✅ test_df: /kaggle/input/grade-school-math-8k-q-a/main_test.csv\ntest_df columns: ['question', 'answer']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                            question  \\\n0  Janet’s ducks lay 16 eggs per day. She eats th...   \n1  A robe takes 2 bolts of blue fiber and half th...   \n\n                                              answer  \n0  Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eg...  \n1  It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nS...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Janet’s ducks lay 16 eggs per day. She eats th...</td>\n      <td>Janet sells 16 - 3 - 4 = &lt;&lt;16-3-4=9&gt;&gt;9 duck eg...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A robe takes 2 bolts of blue fiber and half th...</td>\n      <td>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber\\nS...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"✅ q_col: question\n✅ Created sample_sub with columns: ['id', 'prediction']\nsample_sub columns: ['id', 'prediction']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   id prediction\n0   0           \n1   1           ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"id":"16c344d5-7fcb-4042-892f-89de8d2305fa","cell_type":"code","source":"# =========================\n# 5) Batch inference -> submission.csv\n# =========================\nfrom tqdm import tqdm\nimport math\n\nsub_cols = list(sample_sub.columns)\nid_col_sub = sub_cols[0]\npred_col_sub = sub_cols[1] if len(sub_cols) > 1 else sub_cols[0]\n\nquestions = test_df[q_col].astype(str).tolist()\n\npreds = []\nfor i in tqdm(range(0, len(questions), INFER_BATCH_SIZE), desc=\"Generating\"):\n    batch_q = questions[i:i+INFER_BATCH_SIZE]\n    batch_out = generate_batch(batch_q)\n    preds.extend(batch_out)\n\nsubmission = sample_sub.copy()\n\n# If sample_sub has same row count as test, fill directly. Otherwise align by id if possible.\nif len(submission) == len(preds):\n    submission[pred_col_sub] = preds\nelse:\n    # safer: create a fresh dataframe\n    print(\"⚠️ sample_sub rows != test rows. Writing a new submission with id alignment.\")\n    if id_col_sub in test_df.columns:\n        submission = pd.DataFrame({id_col_sub: test_df[id_col_sub].values, pred_col_sub: preds})\n    else:\n        submission = pd.DataFrame({id_col_sub: list(range(len(preds))), pred_col_sub: preds})\n\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(\"✅ Wrote:\", SUBMISSION_PATH)\ndisplay(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T04:59:16.956982Z","iopub.execute_input":"2025-12-27T04:59:16.957295Z","iopub.status.idle":"2025-12-27T06:45:53.540890Z","execution_failed":"2025-12-27T06:45:55.993Z"}},"outputs":[{"name":"stderr","text":"Generating:  62%|██████▏   | 206/330 [1:46:35<1:04:09, 31.05s/it]\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(questions), INFER_BATCH_SIZE), desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     15\u001b[39m     batch_q = questions[i:i+INFER_BATCH_SIZE]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     batch_out = \u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     preds.extend(batch_out)\n\u001b[32m     19\u001b[39m submission = sample_sub.copy()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mgenerate_batch\u001b[39m\u001b[34m(questions)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m INFER_DO_SAMPLE:\n\u001b[32m     71\u001b[39m     gen_kwargs.update(\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     72\u001b[39m         temperature=\u001b[38;5;28mfloat\u001b[39m(INFER_TEMPERATURE),\n\u001b[32m     73\u001b[39m         top_p=\u001b[38;5;28mfloat\u001b[39m(INFER_TOP_P),\n\u001b[32m     74\u001b[39m         top_k=\u001b[38;5;28mint\u001b[39m(INFER_TOP_K),\n\u001b[32m     75\u001b[39m     ))\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Decode only newly generated tokens (not the prompt)\u001b[39;00m\n\u001b[32m     80\u001b[39m texts = []\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:658\u001b[39m, in \u001b[36mGemma3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m output_hidden_states = (\n\u001b[32m    655\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    656\u001b[39m )\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:570\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    568\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:398\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m residual = hidden_states\n\u001b[32m    397\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_feedforward_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_feedforward_layernorm(hidden_states)\n\u001b[32m    400\u001b[39m hidden_states = residual + hidden_states\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:122\u001b[39m, in \u001b[36mGemma3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[31mKeyboardInterrupt\u001b[39m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null},{"id":"55ebd9ae-3857-4879-816d-1424d706474f","cell_type":"code","source":"# =========================\n# 6) OPTIONAL: Submit via Kaggle CLI + check latest score\n# =========================\nimport os, subprocess\nfrom pathlib import Path\n\ndef detect_competition_slug():\n    # Heuristic: competition slug is usually the directory under /kaggle/input that contains sample_submission.csv\n    sample_paths = sorted(Path(\"/kaggle/input\").rglob(\"sample_submission.csv\"))\n    if not sample_paths:\n        return None\n    # /kaggle/input/<slug>/.../sample_submission.csv -> slug is parts[3]\n    return sample_paths[0].parts[3]\n\nCOMPETITION = detect_competition_slug()\nprint(\"Detected COMPETITION slug:\", COMPETITION)\n\n# Check if kaggle CLI exists\ntry:\n    subprocess.run([\"kaggle\", \"--version\"], check=True, capture_output=True, text=True)\n    kaggle_cli_ok = True\nexcept Exception as e:\n    kaggle_cli_ok = False\n    print(\"❌ kaggle CLI not available:\", e)\n\n# Check credentials\nkaggle_json = Path.home() / \".kaggle\" / \"kaggle.json\"\nhas_creds = kaggle_json.exists() or (os.environ.get(\"KAGGLE_USERNAME\") and os.environ.get(\"KAGGLE_KEY\"))\n\nprint(\"Kaggle credentials present:\", bool(has_creds))\n\nif (not kaggle_cli_ok) or (not has_creds) or (COMPETITION is None):\n    print(\"⚠️ Cannot auto-submit from notebook.\")\n    print(\"   You can still submit manually in Kaggle UI using:\", SUBMISSION_PATH)\nelse:\n    msg = \"Inference submission from merged_lora\"\n    print(\"Submitting...\")\n    subprocess.run(\n        [\"kaggle\", \"competitions\", \"submit\", \"-c\", COMPETITION, \"-f\", str(SUBMISSION_PATH), \"-m\", msg],\n        check=True\n    )\n    print(\"✅ Submitted.\")\n\n    print(\"\\nLatest submissions (top):\")\n    # This prints table including scores when available\n    subprocess.run([\"kaggle\", \"competitions\", \"submissions\", \"-c\", COMPETITION], check=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bc287151-47ef-40f4-84c3-8c1825a58b20","cell_type":"code","source":"# =========================\n# 7) OPTIONAL: Local evaluation (if 'answer' exists)\n# =========================\nimport re\n\ndef extract_final_number(text: str):\n    if text is None:\n        return None\n    s = str(text)\n\n    # GSM8K often uses \"#### <answer>\"\n    if \"####\" in s:\n        tail = s.split(\"####\")[-1].strip()\n        m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", tail.replace(\",\", \"\"))\n        return m.group(0) if m else None\n\n    # fallback: last number anywhere\n    nums = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", s.replace(\",\", \"\"))\n    return nums[-1] if nums else None\n\nif \"answer\" not in test_df.columns:\n    print(\"⚠️ No 'answer' column in test_df; cannot do local accuracy here.\")\nelse:\n    N = min(50, len(test_df))\n    qs = test_df[q_col].astype(str).head(N).tolist()\n    gold = test_df[\"answer\"].astype(str).head(N).tolist()\n\n    pred_texts = []\n    for i in tqdm(range(0, N, INFER_BATCH_SIZE), desc=\"Eval generating\"):\n        pred_texts.extend(generate_batch(qs[i:i+INFER_BATCH_SIZE]))\n\n    pred_nums = [extract_final_number(t) for t in pred_texts]\n    gold_nums = [extract_final_number(t) for t in gold]\n\n    correct = 0\n    for p, g in zip(pred_nums, gold_nums):\n        if (p is not None) and (g is not None) and (p == g):\n            correct += 1\n\n    acc = correct / N if N else 0.0\n    print(f\"✅ Local numeric exact-match on {N} examples: {acc:.3f} ({correct}/{N})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}